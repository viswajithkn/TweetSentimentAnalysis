{"cells":[{"metadata":{},"cell_type":"markdown","source":"As I mention below, this is the first time I am using Bert for any NLP analysis. This is also the first time I have used Pytorch for any Deep Learning work. I thank @ShwetaBaranwal  https://github.com/ShwetaBaranwal/BERT-for-QuestionAnswering/blob/master/BertForQuestionAnswering_from_Transformers.ipynb for the code - I am understanding the algorithm and Pytorch in parallel."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm,trange,tqdm_notebook\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"basePath = '/kaggle/input/tweet-sentiment-extraction/'\nraw_train_data = pd.read_csv(basePath+'train.csv')\nraw_train_data.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class config:\n    BERT_PATH = \"../input/bert-base-uncased/\"    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQ_LENGTH = 160\nTRAIN_BATCH_SIZE = 64\nEVAL_BATCH_SIZE = 16\nTEST_BATCH_SIZE = 16\nLEARNING_RATE = 1e-5\nNUM_TRAIN_EPOCHS = 2\nBERT_TYPE = \"bert-base-uncased\"\nmax_grad_norm = 1.0\nvocab_file = config.BERT_PATH + \"vocab.txt\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_train_data = raw_train_data.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The number of training data points are: ',raw_train_data.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_test_data  = pd.read_csv(basePath+'test.csv')\nraw_test_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The number of testing data points are: ',raw_test_data.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.countplot(raw_train_data['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(raw_test_data['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = (pd.Series(raw_train_data['sentiment']).value_counts(normalize=True, sort=False)*100).plot.bar()\nax.set(ylabel=\"Percent\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = (pd.Series(raw_test_data['sentiment']).value_counts(normalize=True, sort=False)*100).plot.bar()\nax.set(ylabel=\"Percent\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotWordClouds(df_text,sentiment):\n    text = \" \".join(str(tmptext) for tmptext in df_text)\n    text = text.lower()\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=300,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(text)\n  \n    # plot the WordCloud image                        \n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n    plt.title('WordCloud - ' + sentiment)\n    plt.show()         ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subtext = raw_train_data[raw_train_data['sentiment']=='positive']['selected_text']\nfrom wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS) \nplotWordClouds(subtext,'positive')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subtext = raw_train_data[raw_train_data['sentiment']=='neutral']['selected_text']\nplotWordClouds(subtext,'neutral')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subtext = raw_train_data[raw_train_data['sentiment']=='negative']['selected_text']\nplotWordClouds(subtext,'negative')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The wordclouds for the individual sentiments do make sense with what has been tweeted. Just as many others let us try framing this problem as a question, answer and a context problem. This is my first attempt at using BERT for any NLP analysis. Let us see where this leads me."},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_func(out, s_target, e_target):\n    criterion = nn.CrossEntropyLoss()\n    s_loss = criterion(out[0], s_target)\n    e_loss = criterion(out[1], e_target)\n    total_loss = s_loss+e_loss\n    return total_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import nn\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport transformers\nfrom transformers.optimization import AdamW\nimport tokenizers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQ_LENGTH = 160\nTRAIN_BATCH_SIZE = 64\nEVAL_BATCH_SIZE = 16\nLEARNING_RATE = 1e-5\nNUM_TRAIN_EPOCHS = 2\nBERT_TYPE = \"bert-base-uncased\"\nmax_grad_norm = 1.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertBaseQA(nn.Module):\n    def __init__(self, hidden_size, num_labels,conf):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_labels = num_labels\n        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH,config=conf)\n        self.drop_out = nn.Dropout(0.1)\n        self.qa_outputs = nn.Linear(self.hidden_size, self.num_labels)\n        torch.nn.init.normal_(self.qa_outputs.weight, std=0.02)\n    \n    def forward(self, ids, mask, token_ids):\n\n        output = self.bert(\n                          input_ids = ids, \n                          attention_mask = mask,\n                          token_type_ids = token_ids,\n                          )\n    \n        sequence_output = output[0]   #(None, seq_len, hidden_size)\n        sequence_output = self.drop_out(sequence_output)\n        logits = self.qa_outputs(sequence_output) #(None, seq_len, hidden_size)*(hidden_size, 2)=(None, seq_len, 2)\n        start_logits, end_logits = logits.split(1, dim=-1)    #(None, seq_len, 1), (None, seq_len, 1)\n        start_logits = start_logits.squeeze(-1)  #(None, seq_len)\n        end_logits = end_logits.squeeze(-1)    #(None, seq_len)\n\n\n        outputs = (start_logits, end_logits,) \n    \n        return outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertDatasetModule(Dataset):\n    def __init__(self, tokenizer, context, question, max_length, text):\n        self.context = context\n        self.question = question\n        self.text = text\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        'Denotes the total number of samples'\n        return len(self.context)        \n    \n    def __getitem__(self, idx):\n        'Generates one sample of data'\n        context_ = self.context[idx]\n        question_ = self.question[idx]\n        text_ = self.text[idx]\n\n        tok_context = tokenizer.encode(context_)\n        tok_question = tokenizer.encode(question_)\n        tok_answer = tokenizer.encode(text_)\n        \n        context_ids = tok_context.ids\n        question_ids = tok_question.ids\n        answer_ids = tok_answer.ids\n        offsets_orig = tok_context.offsets[1:-1]\n        \n        input_ids = [101] + question_ids[1:-1] + [102] + context_ids[1:-1] + [102]\n        token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n        mask = [1] * len(token_type_ids)        \n        offsets = [(0,0)]*3 + offsets_orig + [(0,0)]\n        \n        \n        s_pos, e_pos = 0, 0\n        for i in range(len(input_ids)):\n            if (input_ids[i: i+len(answer_ids[1:-1])] == answer_ids[1:-1]):\n                s_pos = i\n                e_pos = i + len(answer_ids[1:-1]) - 1\n                break\n            \n        assert((s_pos<len(input_ids)) & (e_pos<len(input_ids)) & (s_pos<=e_pos))\n        pad_length = self.max_length - len(input_ids)\n        if (len(input_ids)<self.max_length):            \n            ids = input_ids +([0]*pad_length)\n        elif (len(input_ids)>self.max_length):\n            ids = input_ids[:self.max_length]\n            \n        if (len(token_type_ids)<self.max_length):\n            token_ids = token_type_ids +([0]*pad_length)\n        elif (len(token_type_ids)>self.max_length):\n            token_ids = token_type_ids[:self.max_length]   \n            \n        if (len(mask)<self.max_length):\n            mask_ids = mask +([0]*pad_length)\n        elif (len(token_type_ids)>self.max_length):\n            mask_ids = mask[:self.max_length]\n            \n        if (len(input_ids)<self.max_length):\n            offsets = offsets + ([(0, 0)] * pad_length)\n        elif (len(input_ids)>self.max_length):\n            offsets = offsets[:self.max_length]            \n        \n        ids = torch.tensor(ids, dtype = torch.long)\n        tt_ids = torch.tensor(token_ids, dtype = torch.long)\n        mask_ids = torch.tensor(mask_ids, dtype = torch.long)\n        offsets = torch.tensor(offsets, dtype = torch.long)\n        start_pos = torch.tensor(s_pos, dtype = torch.long)\n        end_pos = torch.tensor(e_pos, dtype = torch.long)\n        return {'ids': ids,\n            'token_type_ids': tt_ids,\n            'mask':mask_ids,\n            'start_pos': start_pos,\n            'end_pos': end_pos,\n            'offsets': offsets,\n            'context':context_,\n            'question':question_,\n            'text':text_}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_loop(dataloader, model, optimizer, device, max_grad_norm, scheduler=None):\n    model.train()\n    for bi, d in enumerate(tqdm_notebook(dataloader, desc=\"Iteration\")):\n        ids = d['ids']\n        mask_ids = d['mask']\n        token_ids = d['token_type_ids']\n        start_pos = d['start_pos']\n        end_pos = d['end_pos']\n        offsets = d['offsets']\n\n        ids = ids.to(device, dtype = torch.long)\n        mask_ids = mask_ids.to(device, dtype = torch.long)\n        token_ids = token_ids.to(device, dtype = torch.long)\n        start_pos = start_pos.to(device, dtype = torch.long)\n        end_pos = end_pos.to(device, dtype = torch.long)\n\n        optimizer.zero_grad()\n        start_and_end_scores = model(ids, mask_ids, token_ids)\n        # start_scores, end_scores = model(ids, token_ids)\n        loss = loss_func(start_and_end_scores, start_pos, end_pos)\n        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n        loss.backward()\n        optimizer.step()\n        if scheduler is not None:\n          scheduler.step()\n        if bi%100==0:\n          print (f\"bi: {bi}, loss: {loss}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_loop(dataloader, model, device):\n    model.eval()\n    pred_s = None\n    pred_e = None\n    eval_loss = 0.0\n    eval_steps = 0\n\n    for bi, d in enumerate(dataloader):\n        ids = d['ids']\n        mask_ids = d['mask']\n        token_ids = d['token_type_ids']\n        start_pos = d['start_pos']\n        end_pos = d['end_pos']\n        context = d['context']\n        question = d['question']\n        selected_text = d['text']\n        offsets = d['offsets']\n\n        ids = ids.to(device, dtype = torch.long)\n        mask_ids = mask_ids.to(device, dtype = torch.long)\n        token_ids = token_ids.to(device, dtype = torch.long)\n        start_pos = start_pos.to(device, dtype = torch.long)\n        end_pos = end_pos.to(device, dtype = torch.long)\n\n        with torch.no_grad():\n            start_and_end_scores = model(ids,mask_ids, token_ids)\n            loss = loss_func(start_and_end_scores, start_pos, end_pos)\n            eval_loss += loss.mean().item()\n        \n        eval_steps += 1\n        \n        pred_s = torch.softmax(torch.tensor(start_and_end_scores[0]),dim=1).detach().cpu().numpy()\n        pred_e = torch.softmax(torch.tensor(start_and_end_scores[1]),dim=1).detach().cpu().numpy()\n\n    eval_loss = eval_loss/eval_steps\n    pred_start = np.argmax(pred_s, axis=1)\n    pred_end = np.argmax(pred_e, axis=1)\n    \n    jaccards=[]        \n    for i,tweet in enumerate(context):\n        idx_start = pred_start[i]\n        idx_end = pred_end[i]       \n        if idx_end < idx_start:\n            idx_end = idx_start\n        filtered_output  = \"\"        \n        for ix in range(idx_start, idx_end + 1):\n            filtered_output += tweet[offsets[i][ix][0]: offsets[i][ix][1]]\n            if (ix+1) < len(offsets[i]) and offsets[i][ix][1] < offsets[i][ix+1][0]:\n                filtered_output += \" \"\n        \n        if question == \"neutral\" or len(tweet.split()) < 2:\n            filtered_output = tweet        \n        \n        print('Predicted Text:',filtered_output)\n        print('actual text:',selected_text[i])        \n        jaccards.append(jaccard(filtered_output,selected_text[i]))   \n        \n    return eval_loss, pred_start, pred_end, jaccards","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    jaccards=[]        \n    for i in range(len(pred_start)):\n        curText = (tokenizer.decode(ids[i][pred_start[i]:pred_end[i]+1]))\n        origText = d.iloc[i]['text']\n        jaccards.append(jaccard(curText,origText))"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_start = []\ntest_end = []\nmean_jaccard = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = tokenizers.BertWordPieceTokenizer(vocab_file,lowercase=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GroupKFold, StratifiedKFold, train_test_split\nkf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 42)\nfor fold, (tr_ind, val_ind) in enumerate(kf.split(raw_train_data, raw_train_data['sentiment'])):\n    train_data = raw_train_data.iloc[tr_ind].reset_index(drop=True)\n    val_data = raw_train_data.iloc[val_ind].reset_index(drop=True) \n    \n    \n    train_dataset = BertDatasetModule(\n        tokenizer = tokenizer,\n        context = train_data['text'],\n        question = train_data['sentiment'],\n        max_length = MAX_SEQ_LENGTH,\n        text = train_data['selected_text']\n    )\n\n    train_dataloader = DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE, shuffle=True)    \n    \n    eval_dataset = BertDatasetModule(\n        tokenizer = tokenizer,\n        context = val_data['text'],\n        question = val_data['sentiment'],\n        max_length = MAX_SEQ_LENGTH,\n        text = val_data['selected_text']\n    ) \n\n    eval_dataloader = DataLoader(eval_dataset, batch_size = EVAL_BATCH_SIZE, shuffle=False)    \n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(device)\n\n    model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n    model = BertBaseQA(768, 2,model_config).to(device)\n\n    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, correct_bias=False)\n\n    NUM_TRAIN_STEPS = int(len(train_dataset)/TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS) \n    scheduler = transformers.get_constant_schedule_with_warmup(\n                    optimizer, \n                    num_warmup_steps=500,\n                    # num_training_steps=NUM_TRAIN_STEPS,\n                    last_epoch=-1)\n    for epoch in trange(NUM_TRAIN_EPOCHS):\n        train_loop(train_dataloader, model, optimizer, device, max_grad_norm, scheduler)\n        \n    res = eval_loop(eval_dataloader, model, device)\n    val_start = res[1]\n    val_end = res[2]\n    jaccards = res[3]\n              \n    mean_jaccard.append(np.mean(jaccards))\n    torch.save(model.state_dict(),'model_' + str(fold) + '.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_jaccard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_output = []\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\nmodel_0 = BertBaseQA(768, 2,model_config).to(device)\nmodel_1 = BertBaseQA(768, 2,model_config).to(device)\nmodel_2 = BertBaseQA(768, 2,model_config).to(device)\nmodel_0.load_state_dict(torch.load('model_0.pth'))\nmodel_1.load_state_dict(torch.load('model_1.pth'))\nmodel_2.load_state_dict(torch.load('model_2.pth'))\nmodel_0.eval()\nmodel_1.eval()\nmodel_2.eval()\n    \ntest_dataset = BertDatasetModule(\n        tokenizer = tokenizer,\n        context = raw_test_data['text'],\n        question = raw_test_data['sentiment'],\n        max_length = MAX_SEQ_LENGTH,\n        text = raw_test_data['text']\n    ) \ntest_dataloader = DataLoader(test_dataset, batch_size = TEST_BATCH_SIZE, shuffle=False)       \n\nfor bi, d in enumerate(test_dataloader):\n    ids = d['ids']\n    mask_ids = d['mask']\n    token_ids = d['token_type_ids']\n    start_pos = d['start_pos']\n    end_pos = d['end_pos']\n    context = d['context']\n    question = d['question']\n    selected_text = d['text']\n    offsets = d['offsets']\n\n    ids = ids.to(device, dtype = torch.long)\n    mask_ids = mask_ids.to(device, dtype = torch.long)\n    token_ids = token_ids.to(device, dtype = torch.long)\n    start_pos = start_pos.to(device, dtype = torch.long)\n    end_pos = end_pos.to(device, dtype = torch.long)\n    \n    with torch.no_grad():\n        start_and_end_scores0 = model_0(ids,mask_ids, token_ids)        \n        start_and_end_scores1 = model_1(ids,mask_ids, token_ids)        \n        start_and_end_scores2 = model_2(ids,mask_ids, token_ids)        \n        \n        start_scores = (start_and_end_scores0[0]+start_and_end_scores1[0]+start_and_end_scores2[0])/3\n        end_scores = (start_and_end_scores0[1]+start_and_end_scores1[1]+start_and_end_scores2[1])/3\n        \n    pred_s = torch.softmax(torch.tensor(start_scores),dim=1).detach().cpu().numpy()\n    pred_e = torch.softmax(torch.tensor(end_scores),dim=1).detach().cpu().numpy()\n        \n    pred_start = np.argmax(pred_s, axis=1)\n    pred_end = np.argmax(pred_e, axis=1)      \n           \n    for i,tweet in enumerate(context):\n        idx_start = pred_start[i]\n        idx_end = pred_end[i]       \n        if idx_end < idx_start:\n            idx_end = idx_start\n        filtered_output  = \"\"        \n        for ix in range(idx_start, idx_end + 1):\n            filtered_output += tweet[offsets[i][ix][0]: offsets[i][ix][1]]\n            if (ix+1) < len(offsets[i]) and offsets[i][ix][1] < offsets[i][ix+1][0]:\n                filtered_output += \" \"\n        print(filtered_output)\n        if question == \"neutral\" or len(tweet.split()) < 2:\n            filtered_output = tweet\n        predicted_output.append(filtered_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_start","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_end","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = predicted_output\nsample.to_csv(\"submission.csv\", index=False)\nsample.head(30)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}